{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25780,"status":"ok","timestamp":1713209507607,"user":{"displayName":"Kyle Doherty","userId":"02054862309059506145"},"user_tz":420},"id":"YpKwhuOywr-6","outputId":"2173ae38-dd1a-4a78-aee2-16631ca6d863"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: jax 0.4.26\n","Uninstalling jax-0.4.26:\n","  Successfully uninstalled jax-0.4.26\n","Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n","Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n","  Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Add token as git credential? (Y/n) Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n","\n"]}],"source":["!pip uninstall -y jax jaxlib\n","!pip install -Uqq datasets optuna kaleido\n","\n","from google.colab import userdata, runtime\n","import subprocess\n","\n","hf_token = userdata.get('hf_token')\n","input_str = f'{hf_token}\\nn\\n'\n","result = subprocess.run(['huggingface-cli', 'login'], input=input_str, text=True, capture_output=True)\n","print(result.stdout)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cYwUFhVWwlpJ","outputId":"2161b164-a2c2-447f-e6e8-fce2296ef0c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Learning rate: 8.113530089356352e-06\n","Batch size: 96\n","Number of available CPU cores: 8\n","Applying preprocessing transforms...\n","Seed: 1984\n","Loading model...\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1/27\n","Epoch: 2/27\n","Epoch: 3/27\n","Epoch: 4/27\n","Epoch: 5/27\n","Epoch: 6/27\n","Epoch: 7/27\n","Epoch: 8/27\n","Epoch: 9/27\n","Epoch: 10/27\n","Epoch: 11/27\n","Epoch: 12/27\n","Epoch: 13/27\n","Epoch: 14/27\n","Epoch: 15/27\n","Epoch: 16/27\n","Epoch: 17/27\n","Epoch: 18/27\n","Epoch: 19/27\n","Epoch: 20/27\n","Epoch: 21/27\n","Epoch: 22/27\n","Epoch: 23/27\n","Epoch: 24/27\n","Epoch: 25/27\n","Epoch: 26/27\n","Epoch: 27/27\n"]}],"source":["from functools import reduce\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, default_collate\n","from torchvision.transforms import Resize, Normalize, ToTensor, Compose, transforms, CenterCrop, RandomCrop, RandomChoice\n","from torchvision.transforms.v2 import CutMix\n","from sklearn.metrics import f1_score\n","import optuna\n","import pickle\n","import json\n","import colorsys\n","import math\n","\n","from datasets import load_dataset, concatenate_datasets\n","\n","seed = 1984\n","\n","# Device setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","focus = 'models'\n","results_dir = f'/content/drive/MyDrive/Colab_Notebooks/dye_test_opt/ternary/results/{focus}'\n","os.makedirs(results_dir, exist_ok=True)\n","\n","studies = ['learning', 'synthetic_properties', 'augs']\n","best_params = {}\n","\n","for study in studies:\n","    study_path = results_dir.replace(f'{focus}', f'{study}/study.pkl')\n","    with open(study_path, 'rb') as f:\n","        study = pickle.load(f)\n","    best_trial = study.best_trial\n","    best_params.update(best_trial.params)\n","\n","\n","epoch_path = results_dir.replace(f'{focus}', 'epoch_count/epoch_result.json')\n","with open(epoch_path, 'r') as f:\n","    epoch_result = json.load(f)\n","n_epochs = epoch_result['highest_f1_epoch']\n","\n","# Extract the learning rate and batch size\n","lr = best_params['lr']\n","bs = best_params['bs']\n","print(f'Learning rate: {lr}')\n","print(f'Batch size: {bs}')\n","\n","# Load dataset\n","ds = load_dataset('mpg-ranch/dye_test', split='train')\n","\n","# Dataset preparation\n","total_samples = len(ds)\n","n_workers = os.cpu_count()\n","print(f'Number of available CPU cores: {n_workers}')\n","context_sz = 154\n","canvas_sz = context_sz + 14*6 # 1x1 meter context\n","\n","imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n","                  'std': [0.229, 0.224, 0.225]}\n","\n","# Preprocessing transforms\n","preprocs = Compose([\n","    CenterCrop((canvas_sz, canvas_sz))\n","])\n","\n","def preproc_transforms(examples):\n","    examples[\"img\"] = [preprocs(image.convert(\"RGB\")) for image in examples[\"image\"]]\n","    return examples\n","\n","print(\"Applying preprocessing transforms...\")\n","ds = ds.map(preproc_transforms, remove_columns=[\"image\",\"color\",\"size\",\"concentration\"], batched=True, batch_size=len(ds))\n","n_classes = len(np.unique(ds['label']))\n","\n","# Define model loading function\n","def load_model(arch, n_classes):\n","    print(\"Loading model...\")\n","    model = torch.hub.load('facebookresearch/dinov2', arch)\n","    num_ftrs = model.norm.normalized_shape[0]\n","    model.head = nn.Linear(num_ftrs, n_classes)\n","    model.to(device)\n","    return model\n","\n","def modified_f1_score(labels, predictions):\n","    scores = []\n","\n","    # For 1 vs. 0, exclude class 2 and consider only 1 and 0 for binary comparison\n","    mask_1_vs_0 = (labels != 2)\n","    labels_1_vs_0 = labels[mask_1_vs_0] == 1\n","    predictions_1_vs_0 = predictions[mask_1_vs_0] == 1\n","    if np.any(labels_1_vs_0) or np.any(predictions_1_vs_0):\n","        f1_1_vs_0 = f1_score(labels_1_vs_0, predictions_1_vs_0, pos_label=True, average='binary')\n","        scores.append(f1_1_vs_0)\n","\n","    # For 2 vs. 0, exclude class 1 and consider only 2 and 0 for binary comparison\n","    mask_2_vs_0 = (labels != 1)\n","    labels_2_vs_0 = labels[mask_2_vs_0] == 2\n","    predictions_2_vs_0 = predictions[mask_2_vs_0] == 2\n","    if np.any(labels_2_vs_0) or np.any(predictions_2_vs_0):\n","        f1_2_vs_0 = f1_score(labels_2_vs_0, predictions_2_vs_0, pos_label=True, average='binary')\n","        scores.append(f1_2_vs_0)\n","\n","    # Calculate the mean of the F1 scores if any valid scores were calculated\n","    mean_f1 = np.mean(scores) if scores else 0.0\n","\n","    return f1_1_vs_0, f1_2_vs_0, mean_f1\n","\n","class SuperimposeSquare(object):\n","    def __init__(self, red_hue=0.83, blue_hue=0.45,\n","                 red_value=0.4, blue_value=0.4,\n","                 red_saturation=0.4, blue_saturation=0.4,\n","                 max_opacity=0.3, min_opacity=0.1):\n","        self.red_hue = red_hue\n","        self.blue_hue = blue_hue\n","        self.red_value = red_value\n","        self.blue_value = blue_value\n","        self.red_saturation = red_saturation\n","        self.blue_saturation = blue_saturation\n","        self.max_opacity = max(0, min(1, max_opacity))\n","        self.min_opacity = max(0, min(1, min_opacity))\n","\n","    def __call__(self, tensor):\n","      image = tensor.unsqueeze(0)\n","      h, w = image.size()[-2:]\n","\n","      # Randomly choose between small and large box sizes\n","      small_box = random.choice([True, False])\n","      if small_box:\n","          mask_size = 15\n","      else:\n","          mask_size = 77\n","\n","      color_choice = random.choice(['blue', 'red'])\n","      if color_choice == 'red':\n","          hue = self.red_hue\n","          value = self.red_value\n","          saturation = self.red_saturation\n","          label = 2\n","      else:\n","          hue = self.blue_hue\n","          value = self.blue_value\n","          saturation = self.blue_saturation\n","          label = 1\n","\n","      saturation = 1.0  # Full saturation for vivid colors\n","      color_rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n","      color_tensor = torch.tensor(color_rgb)\n","\n","      x = (w - mask_size) // 2  # Centering the square on x-axis\n","      y = (h - mask_size) // 2  # Centering the square on y-axis\n","\n","      square = color_tensor.view(3, 1, 1).expand(-1, mask_size, mask_size)\n","      opacity = random.uniform(self.min_opacity, self.max_opacity)\n","      square = opacity * square + (1 - opacity) * image[:, :, y:y+mask_size, x:x+mask_size]\n","      image[:, :, y:y+mask_size, x:x+mask_size] = square\n","\n","      return image.squeeze(0), label\n","\n","# Hyperparameters for augmentations\n","synthetic_prob = best_params['synthetic_prob']\n","random_crop_prob = best_params['random_crop_prob']\n","random_horizontal_flip = best_params['random_horizontal_flip']\n","random_vertical_flip = best_params['random_vertical_flip']\n","random_rotation = best_params['random_rotation']\n","brightness = best_params['brightness']\n","contrast = best_params['contrast']\n","saturation = best_params['saturation']\n","hue = best_params['hue']\n","\n","# Train transforms\n","train_tfms = Compose([\n","    RandomChoice([RandomCrop(size=context_sz), CenterCrop(context_sz)], p=[random_crop_prob, 1 - random_crop_prob]),\n","    transforms.RandomHorizontalFlip(p=0.5 if random_horizontal_flip else 0),\n","    transforms.RandomVerticalFlip(p=0.5 if random_vertical_flip else 0),\n","    transforms.RandomRotation(random_rotation),\n","    transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue),\n","    #ToTensor applied in collate_fn\n","    Normalize(mean=imagenet_stats['mean'], std=imagenet_stats['std'])  # Normalize using ImageNet stats\n","])\n","\n","val_tfms = Compose([\n","    CenterCrop(size=context_sz),\n","    ToTensor(),\n","    Normalize(mean=imagenet_stats['mean'], std=imagenet_stats['std'])  # Normalize using ImageNet stats\n","])\n","\n","def batch_tfms_val(examples):\n","    examples[\"img\"] = [val_tfms(image) for image in examples[\"img\"]]\n","    return examples\n","\n","f1_scores = {epoch: [] for epoch in range(1, n_epochs + 1)}\n","\n","# Seed loop\n","print(f\"Seed: {seed}\")\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","# Model, optimizer, and loss function setup\n","model = load_model('dinov2_vitb14', n_classes)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","def collate_fun(batch):\n","  imgs = [ToTensor()(item['img']) for item in batch]  # Convert PIL images to tensors\n","  imgs = torch.stack(imgs)\n","  labels = torch.tensor([item['label'] for item in batch])\n","\n","  new_imgs = []\n","  new_labels = []\n","\n","  for img, label in zip(imgs, labels):\n","      if label == 0 and synthetic_prob > random.random():\n","          transformed_img, new_label = SuperimposeSquare(best_params['red_hue'],\n","                                                          best_params['blue_hue'],\n","                                                          best_params['red_value'],\n","                                                          best_params['blue_value'],\n","                                                          best_params['red_saturation'],\n","                                                          best_params['blue_saturation'],\n","                                                          best_params['max_opacity'],\n","                                                          best_params['min_opacity']\n","                                                          )(img)\n","          transformed_img = train_tfms(transformed_img)\n","          new_imgs.append(transformed_img)\n","          new_labels.append(new_label)\n","      else:\n","          img = train_tfms(img)\n","          new_imgs.append(img)\n","          new_labels.append(label)\n","\n","  imgs = torch.stack(new_imgs)\n","  labels = torch.tensor(new_labels)\n","  # Wrap the result in a dictionary\n","  return {'img': imgs, 'label': labels}\n","\n","train_loader = DataLoader(ds, batch_size=bs, shuffle=True, num_workers=n_workers, collate_fn=collate_fun)\n","\n","# Training loop\n","for epoch in range(1, n_epochs +1):\n","    print(f\"Epoch: {epoch}/{n_epochs}\")\n","    model.train()\n","    for _, data in enumerate(train_loader):\n","        inputs, labels = data['img'].to(device), data['label'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","model.eval()\n","torch.save(model.state_dict(), os.path.join(results_dir,  'test.pth'))\n","\n","runtime.unassign()"]},{"cell_type":"code","source":["runtime.unassign()"],"metadata":{"id":"ZwmyqcCBvl8S"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[],"mount_file_id":"1UjqhDN2DllHYAwNKQQJtdzMjGFEnW3Go","authorship_tag":"ABX9TyO1Yvz0zvRVeYal/SRoxsr1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}