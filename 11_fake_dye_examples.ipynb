{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5764,"status":"ok","timestamp":1713540552226,"user":{"displayName":"Kyle Doherty","userId":"02054862309059506145"},"user_tz":420},"id":"TO4WjnSA8jlO"},"outputs":[],"source":["!pip install -Uqq datasets optuna"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1309,"status":"ok","timestamp":1713541159511,"user":{"displayName":"Kyle Doherty","userId":"02054862309059506145"},"user_tz":420},"id":"r0MZP_Lo8FrL","outputId":"9e21fc25-9203-4bf9-ddc2-61cff3e113c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Add token as git credential? (Y/n) Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n","\n"]}],"source":["import torch\n","from torchvision import transforms\n","from PIL import Image, ImageDraw\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","from datasets import load_dataset\n","\n","from google.colab import userdata, runtime\n","import subprocess\n","\n","from torch.utils.data import DataLoader, default_collate\n","from torchvision.transforms import Resize, Normalize, ToTensor, Compose, transforms, CenterCrop, RandomCrop, RandomChoice\n","import torch.nn.functional as F\n","import random\n","\n","import torch\n","import random\n","import colorsys\n","import math\n","\n","import pickle\n","import optuna\n","\n","hf_token = userdata.get('hf_token')\n","input_str = f'{hf_token}\\nn\\n'\n","result = subprocess.run(['huggingface-cli', 'login'], input=input_str, text=True, capture_output=True)\n","print(result.stdout)\n","\n","seed = 1984\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Yk4gPheGrYWKRdZSgeOiTFa1P6WcE08P"},"id":"42zohsd6_OpK","executionInfo":{"status":"ok","timestamp":1713541221544,"user_tz":420,"elapsed":7596,"user":{"displayName":"Kyle Doherty","userId":"02054862309059506145"}},"outputId":"5107b3c5-b937-44bd-9e12-c2c3d30a1dd5"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["study_path = '/content/drive/MyDrive/Colab_Notebooks/dye_test_opt/ternary/results/synthetic_properties/study.pkl'\n","\n","with open(study_path, 'rb') as f:\n","    study = pickle.load(f)\n","    best_params = study.best_params\n","\n","class SuperimposeSquare(object):\n","    def __init__(self, red_hue=0.83, blue_hue=0.45,\n","                 red_value=0.4, blue_value=0.4,\n","                 red_saturation=0.4, blue_saturation=0.4,\n","                 max_opacity=0.3, min_opacity=0.1):\n","        self.red_hue = red_hue\n","        self.blue_hue = blue_hue\n","        self.red_value = red_value\n","        self.blue_value = blue_value\n","        self.red_saturation = red_saturation\n","        self.blue_saturation = blue_saturation\n","        self.max_opacity = max(0, min(1, max_opacity))\n","        self.min_opacity = max(0, min(1, min_opacity))\n","\n","    def __call__(self, tensor):\n","      image = tensor.unsqueeze(0)\n","      h, w = image.size()[-2:]\n","\n","      # Randomly choose between small and large box sizes\n","      small_box = random.choice([True, False])\n","      if small_box:\n","          mask_size = 15\n","      else:\n","          mask_size = 77\n","\n","      color_choice = random.choice(['blue', 'red'])\n","      if color_choice == 'red':\n","          hue = self.red_hue\n","          value = self.red_value\n","          saturation = self.red_saturation\n","          label = 2\n","      else:\n","          hue = self.blue_hue\n","          value = self.blue_value\n","          saturation = self.blue_saturation\n","          label = 1\n","\n","      saturation = 1.0  # Full saturation for vivid colors\n","      color_rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n","      color_tensor = torch.tensor(color_rgb)\n","\n","      x = (w - mask_size) // 2  # Centering the square on x-axis\n","      y = (h - mask_size) // 2  # Centering the square on y-axis\n","\n","      square = color_tensor.view(3, 1, 1).expand(-1, mask_size, mask_size)\n","      opacity = random.uniform(self.min_opacity, self.max_opacity)\n","      square = opacity * square + (1 - opacity) * image[:, :, y:y+mask_size, x:x+mask_size]\n","      image[:, :, y:y+mask_size, x:x+mask_size] = square\n","\n","      return image.squeeze(0), label\n","\n","context_sz = 154\n","\n","# Load the dataset\n","ds = load_dataset('mpg-ranch/dye_test', split='train')\n","\n","# Preprocessing transforms\n","preprocs = Compose([\n","    CenterCrop((context_sz, context_sz)),  # saving croping for sub-sample loop\n","])\n","\n","def preproc_transforms(examples):\n","    examples[\"img\"] = [preprocs(image.convert(\"RGB\")) for image in examples[\"image\"]]\n","    return examples\n","\n","# Apply the preprocessing transforms\n","ds = ds.map(preproc_transforms, remove_columns=[\"image\", \"color\", \"size\", \"concentration\"], batched=True, batch_size=len(ds))\n","\n","batch_size = 8\n","\n","# Apply the SquareOverlay transform to the batch\n","transform = transforms.Compose([\n","    ToTensor()\n","])\n","\n","def collate_fun(batch):\n","    imgs = [transform(item['img']) for item in batch]\n","    labels = [item['label'] for item in batch]\n","\n","    new_imgs = []\n","    new_labels = []\n","\n","    for img, label in zip(imgs, labels):\n","        if label == 0:\n","            transformed_img, new_label = SuperimposeSquare(best_params['red_hue'],\n","                                                            best_params['blue_hue'],\n","                                                            best_params['red_value'],\n","                                                            best_params['blue_value'],\n","                                                            best_params['red_saturation'],\n","                                                            best_params['blue_saturation'],\n","                                                            best_params['max_opacity'],\n","                                                            best_params['min_opacity']\n","                                                           )(img)\n","            new_imgs.append(transformed_img)\n","            new_labels.append(new_label)\n","        else:\n","            new_imgs.append(img)\n","            new_labels.append(label)\n","\n","    imgs = torch.stack(new_imgs)\n","    labels = torch.tensor(new_labels)\n","\n","    return {'img': imgs, 'label': labels}\n","\n","# Create a balanced dataset with equal numbers of label 0 and label > 0\n","label_0_ds = ds.filter(lambda x: x['label'] == 0)\n","\n","# Now combine the two datasets, making sure there are equal numbers of each\n","# Assuming we want 8 of each to make a batch of 16 for the plot\n","balanced_ds = torch.utils.data.ConcatDataset([\n","    torch.utils.data.Subset(label_0_ds, range(16)),\n","])\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","dataloader = DataLoader(balanced_ds, batch_size=16, shuffle=True, collate_fn=collate_fun)\n","\n","colors = ['blue','red']\n","\n","# Function to plot images in a grid\n","def plot_images(images, labels, rows=4, cols=4):\n","    # Increase the size of the subplot for better spacing control\n","    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n","\n","    # Loop through all the plots in the grid\n","    for i, ax in enumerate(axes.flat):\n","        if i < len(images):\n","            image = images[i].permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n","            ax.imshow(image.numpy())  # Show image\n","            #ax.set_title(f'{colors[labels[i]-1]}')  # Set title with the color label\n","            ax.axis('off')  # Hide axes\n","\n","    # Adjust layout to be tighter\n","    plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.05, hspace=0.05)\n","\n","    # Apply tight layout with reduced padding\n","    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","    plt.show()\n","\n","# Get a batch of images\n","dataiter = iter(dataloader)\n","batch = next(dataiter)\n","images, labels = batch['img'], batch['label']\n","\n","# Plot the images\n","plot_images(images, labels)"]},{"cell_type":"code","source":[],"metadata":{"id":"T1f5V95C_HGE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1-kuL4HnQOdZzsi1802ZMMyzlWQPaTbVq","authorship_tag":"ABX9TyO8DZRIFDhta/F9p7NEzdDR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}